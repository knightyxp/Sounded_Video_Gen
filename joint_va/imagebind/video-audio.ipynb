{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/JmhWMRN_DT0#3408#3418.wav', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/bUntI90An-U#2540#2550.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/U22mj35rYZM#2997#3007.wav']\n",
    "\n",
    "video_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/JmhWMRN_DT0#3408#3418.mp4', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/bUntI90An-U#2540#2550.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water/U22mj35rYZM#2997#3007.mp4']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(type(embeddings))\n",
    "print(embeddings.keys())\n",
    "print(embeddings[ModalityType.VISION].shape)\n",
    "print(embeddings[ModalityType.AUDIO].shape)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"320\" height=\"240\" controls>\n",
    "#   <source src=video_paths[0] type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\")\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <audio controls>\n",
    "#   <source src=audio_paths[0]>\n",
    "# </audio>\n",
    "# \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/-4btXeiiBek#3207#3217.wav', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/1i79N_6wuIA#5680#5690.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/d4Q_1UegaQ0#9067#9077.wav']\n",
    "\n",
    "video_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/-4btXeiiBek#3207#3217.mp4', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/1i79N_6wuIA#5680#5690.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/underwater_bubbling/d4Q_1UegaQ0#9067#9077.mp4']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "print(inputs[ModalityType.VISION].shape)\n",
    "print(inputs[ModalityType.AUDIO].shape)\n",
    "exit()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(type(embeddings))\n",
    "print(embeddings.keys())\n",
    "print(embeddings[ModalityType.VISION].shape)\n",
    "print(embeddings[ModalityType.AUDIO].shape)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# HTML(\"\"\"\n",
    "# <video width=\"320\" height=\"240\" controls>\n",
    "#   <source src=video_paths[0] type=\"video/mp4\">\n",
    "# </video>\n",
    "# \"\"\")\n",
    "\n",
    "# HTML(\"\"\"\n",
    "# <audio controls>\n",
    "#   <source src=audio_paths[0]>\n",
    "# </audio>\n",
    "# \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water_processed/JmhWMRN_DT0#3408#3418_clip.wav', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water_processed/bUntI90An-U#2540#2550_clip.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/fire_crackling_processed/_deFeO_gnwc#178#188_clip.wav']\n",
    "\n",
    "video_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water_processed/JmhWMRN_DT0#3408#3418_clip.mp4', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/splashing_water_processed/bUntI90An-U#2540#2550_clip.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/fire_crackling_processed/_deFeO_gnwc#178#188_clip.mp4']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(type(embeddings))\n",
    "print(embeddings.keys())\n",
    "print(embeddings[ModalityType.VISION].shape)\n",
    "print(embeddings[ModalityType.AUDIO].shape)\n",
    "\n",
    "# print(\n",
    "#     \"Vision x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "# print(\n",
    "#     \"Audio x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# from IPython.display import Video\n",
    "# Video(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/qiaogu.wav', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/jiazigu.wav']\n",
    "\n",
    "video_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/qiaogu.mp4', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/jiazigu.mp4']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(type(embeddings))\n",
    "print(embeddings.keys())\n",
    "print(embeddings[ModalityType.VISION].shape)\n",
    "print(embeddings[ModalityType.AUDIO].shape)\n",
    "\n",
    "# print(\n",
    "#     \"Vision x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "# print(\n",
    "#     \"Audio x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "# from IPython.display import Video\n",
    "# Video(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-01.wav', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-02.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-03.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-04.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-05.wav',\n",
    "                # '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-06.wav',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-07.wav']\n",
    "\n",
    "video_paths = ['/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-01.mp4', \n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-02.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-03.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-04.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-05.mp4',\n",
    "                # '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-06.mp4',\n",
    "                '/home/yazhou/disk1/projects/edit/dataset/landscape/train/own/kitchen-07.mp4']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    # ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device,clip_duration=3, clips_per_video=20),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device, clip_duration=3, clips_per_video=20),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(type(embeddings))\n",
    "print(embeddings.keys())\n",
    "print(embeddings[ModalityType.VISION].shape)\n",
    "print(embeddings[ModalityType.AUDIO].shape)\n",
    "\n",
    "# print(\n",
    "#     \"Vision x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "# print(\n",
    "#     \"Audio x Text: \",\n",
    "#     torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    "# )\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Audio x Vision: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.VISION].T, dim=-1),\n",
    ")\n",
    "\n",
    "\n",
    "# from IPython.display import Video\n",
    "# Video(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('glv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43ac2505129915c61c0b7e29a02476ff17a85fdb675db96f4fe5bb750a175390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
